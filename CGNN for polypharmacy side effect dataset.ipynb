{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load sample of polypharmacy side effect dataset\n",
    "Every element of sample is node pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "\n",
    "file_sample = open(\".\\\\polypharmacy side effect dataset\\\\sample.txt\").readlines()\n",
    "\n",
    "for line in file_sample:\n",
    "    \n",
    "    sample.append((int(line.split(\" \")[0]), int(line.split(\" \")[1])))\n",
    "    \n",
    "del file_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load conduit type group for sample\n",
    "1 denotes the samples in rank from 1 to 30k;\n",
    "\n",
    "2 denotes the samples in rank from 30k to 50k;\n",
    "\n",
    "3 denotes the samples in rank from 50k to 63472."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = []\n",
    "\n",
    "file_group = open(\".\\\\polypharmacy side effect dataset\\\\group.txt\").readlines()\n",
    "\n",
    "for line in file_group:\n",
    "    \n",
    "    group.append(int(line))\n",
    "    \n",
    "del file_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load adjacent matrix and build norm adjacent matrix of drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = np.load(\".\\\\polypharmacy side effect dataset\\\\adj_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = adj_matrix.shape[0]\n",
    "\n",
    "degree_matrix = np.zeros((node_num, node_num))\n",
    "\n",
    "for i in range(node_num):\n",
    "        \n",
    "    degree_matrix[i,i] = np.sum(adj_matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_degree_matrix = np.zeros((node_num, node_num))\n",
    "\n",
    "for i in range(node_num):\n",
    "\n",
    "    norm_degree_matrix[i, i] = degree_matrix[i, i]**(-1 / 2)\n",
    "\n",
    "middel_adj_matrix = np.dot(norm_degree_matrix, adj_matrix)\n",
    "\n",
    "norm_adj_matrix = np.dot(middel_adj_matrix, norm_degree_matrix)\n",
    "\n",
    "del middel_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj_matrix = torch.from_numpy(norm_adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the degree of drug node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = []\n",
    "\n",
    "for i in range(node_num):\n",
    "    \n",
    "    degree.append(degree_matrix[i, i])\n",
    "    \n",
    "del degree_matrix\n",
    "\n",
    "del adj_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage1: node learning\n",
    "in_size is the dimension of the features of the previous layer ($l-1$).\n",
    "\n",
    "out_size is the dimension of the feature of the current layer ($l$).\n",
    "\n",
    "old_node_embedding is the feature of the previous layer ($l-1$).\n",
    "\n",
    "adj_mat is the norm adjacent matrix.\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node_learning(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_size,\n",
    "                 out_size):\n",
    "\n",
    "        super(node_learning, self).__init__()\n",
    "\n",
    "        self.nei_update1 = nn.Linear(in_size, out_size, bias=False)\n",
    "\n",
    "        self.self_node_update1 = nn.Linear(in_size, out_size, bias=False)\n",
    "         \n",
    "    def forward(self,\n",
    "                out_size,\n",
    "                adj_mat,\n",
    "                old_node_embedding,\n",
    "                use_divce):\n",
    "        \n",
    "        total_node = old_node_embedding.shape[0]\n",
    "    \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            new_node_embedding = torch.zeros((total_node, out_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_node_embedding = torch.zeros((total_node, out_size)).cpu()\n",
    "            \n",
    "        for node in range(total_node):\n",
    "            \n",
    "            nei_information, nei_gather = 0, 0\n",
    "            \n",
    "            self_information, total_information = 0, 0\n",
    "                    \n",
    "            self_information += self.self_node_update1(\n",
    "                        old_node_embedding[node, :])\n",
    "                    \n",
    "            nei_gather += torch.mm(adj_mat[node, :].reshape(1, -1),\n",
    "                                           old_node_embedding)\n",
    "                    \n",
    "            nei_information += self.nei_update1(nei_gather)\n",
    "                    \n",
    "            total_information += F.relu(nei_information +\n",
    "                                                       self_information)\n",
    "                \n",
    "            new_node_embedding[node, :] = torch.add(\n",
    "                new_node_embedding[node, :], total_information)\n",
    "            \n",
    "        return new_node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2: conduit node learning\n",
    "pre_size is the dimension of the features of the previous layer ($l-1$).\n",
    "\n",
    "next_size is the dimension of the feature of the current layer ($l$).\n",
    "\n",
    "node_embedding is the feature matrix of drug node.\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduit_node_learning(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pre_size,\n",
    "                 next_size):\n",
    "\n",
    "        super(conduit_node_learning, self).__init__()\n",
    "\n",
    "        self.left_gate1 = nn.Linear(pre_size, next_size, bias=False)\n",
    "\n",
    "        self.right_gate1 = nn.Linear(pre_size, next_size, bias=False)\n",
    "\n",
    "        self.left_gate2 = nn.Linear(pre_size, next_size, bias=False)\n",
    "        \n",
    "        self.right_gate2 = nn.Linear(pre_size, next_size, bias=False)\n",
    "        \n",
    "        self.left_gate3 = nn.Linear(pre_size, next_size, bias=False)\n",
    "        \n",
    "        self.right_gate3 = nn.Linear(pre_size, next_size, bias=False)\n",
    "        \n",
    "        self.c1_conduit_update = nn.Linear(pre_size,\n",
    "                                                  next_size,\n",
    "                                                  bias=True)\n",
    "        \n",
    "        self.c2_conduit_update = nn.Linear(pre_size,\n",
    "                                                   next_size,\n",
    "                                                   bias=True)\n",
    "        \n",
    "        self.c3_conduit_update = nn.Linear(pre_size,\n",
    "                                            next_size,\n",
    "                                            bias=True)\n",
    "    \n",
    "    def forward(self, \n",
    "                sample,\n",
    "                group,\n",
    "                next_size, \n",
    "                node_embedding, \n",
    "                use_divce):\n",
    "        \n",
    "        conduit_sample = sample\n",
    "        \n",
    "        index = list(range(len(conduit_sample)))\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros(\n",
    "                (len(conduit_sample), next_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros(\n",
    "                (len(conduit_sample), next_size)).cpu()\n",
    "            \n",
    "        for j in index:\n",
    "            \n",
    "            i = conduit_sample[j]\n",
    "            \n",
    "            left_gate, right_gate, conduit_embedding = 0, 0, 0\n",
    "            \n",
    "            middle_state, gather_information = 0, 0\n",
    "            \n",
    "            gather_information += torch.add(node_embedding[i[0], :],\n",
    "                                            node_embedding[i[1], :])\n",
    "            \n",
    "            if group[j] == 1:\n",
    "                \n",
    "                left_gate += torch.sigmoid(\n",
    "                    self.left_gate1(node_embedding[i[0], :]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(\n",
    "                    self.right_gate1(node_embedding[i[1], :]))\n",
    "                \n",
    "                middle_state += torch.tanh(\n",
    "                    self.c1_conduit_update(gather_information))\n",
    "                \n",
    "            if group[j] == 2:\n",
    "                \n",
    "                left_gate += torch.sigmoid(\n",
    "                    self.left_gate2(node_embedding[i[0], :]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(\n",
    "                    self.right_gate2(node_embedding[i[1], :]))\n",
    "                \n",
    "                middle_state += torch.tanh(\n",
    "                    self.c2_conduit_update(gather_information))\n",
    "                \n",
    "            if group[j] == 3:\n",
    "                \n",
    "                left_gate += torch.sigmoid(\n",
    "                    self.left_gate3(node_embedding[i[0], :]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(\n",
    "                    self.right_gate3(node_embedding[i[1], :]))\n",
    "                \n",
    "                middle_state += torch.tanh(\n",
    "                    self.c3_conduit_update(gather_information))\n",
    "            \n",
    "            conduit_embedding += left_gate * middle_state + right_gate * middle_state\n",
    "            \n",
    "            new_conduit_embedding[j, :] += conduit_embedding.reshape(-1)\n",
    "       \n",
    "        return new_conduit_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The framework of CGNN\n",
    "The dimensions of each layer are the same as those in paper.\n",
    "\n",
    "pre_node_embedding is the initial feature of drug node.\n",
    "\n",
    "adj_mat is the norm adjacent matrix.\n",
    "\n",
    "self.gather_1 is the weight matrix of layer-wise updating rule.\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduitGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(conduitGNN, self).__init__()\n",
    "        \n",
    "        self.node_update_layer1 = node_learning(256, 128)\n",
    "        \n",
    "        self.conduit_layer1 = conduit_node_learning(128, 964)\n",
    "        \n",
    "        self.node_update_layer2 = node_learning(128, 64)\n",
    "        \n",
    "        self.conduit_layer2 = conduit_node_learning(64, 964)\n",
    "        \n",
    "        self.gather_1 = nn.Linear(964, 964, bias=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                pre_node_embedding,\n",
    "                adj_mat,\n",
    "                sample,\n",
    "                group,\n",
    "                use_divce):\n",
    "        \n",
    "        use_divce = use_divce\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            output = torch.zeros((len(sample), 964)).cuda()\n",
    "            \n",
    "            pre_node_embedding = pre_node_embedding.cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            output = torch.zeros((len(sample), 964)).cpu()\n",
    "            \n",
    "            pre_node_embedding = pre_node_embedding.cpu()\n",
    "            \n",
    "        ##########\n",
    "\n",
    "        node_embedding_1 = self.node_update_layer1(128,\n",
    "                                                   adj_mat,\n",
    "                                                   pre_node_embedding,\n",
    "                                                   use_divce)\n",
    "\n",
    "        conduit_embedding_1 = self.conduit_layer1(sample, \n",
    "                                                  group,\n",
    "                                                  964,\n",
    "                                                  node_embedding_1, \n",
    "                                                  use_divce)\n",
    "        \n",
    "        #########\n",
    "        node_embedding_2 = self.node_update_layer2(64,\n",
    "                                                   adj_mat,\n",
    "                                                   node_embedding_1,\n",
    "                                                   use_divce)\n",
    "        \n",
    "        conduit_embedding_2 = self.conduit_layer2(sample,\n",
    "                                                  group,\n",
    "                                                  964,\n",
    "                                                  node_embedding_2,\n",
    "                                                  use_divce)\n",
    "        #########\n",
    "        \n",
    "        \n",
    "        gather_conduit1 = torch.sigmoid(\n",
    "            self.gather_1(conduit_embedding_1) + conduit_embedding_2)\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        output += gather_conduit1\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing acc for each side effect and average all acc as final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_acc(p_mat, plabel_mat, nlabel_mat):\n",
    "    \n",
    "    pre_p = (p_mat > 0.5).double()*plabel_mat\n",
    "    \n",
    "    pre_n = (p_mat <= 0.5).double()*nlabel_mat\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    for c in range(p_mat.shape[1]):\n",
    "        \n",
    "        tp_num = torch.sum(pre_p[:, c])\n",
    "        \n",
    "        edge1_num = torch.sum(plabel_mat[:, c])\n",
    "        \n",
    "        tn_num = torch.sum(pre_n[:, c])\n",
    "        \n",
    "        edge2_num = torch.sum(nlabel_mat[:, c])\n",
    "        \n",
    "        acc = (tp_num + tn_num)/(edge1_num + edge2_num)\n",
    "        \n",
    "        acc_list.append(acc.item())\n",
    "        \n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute loss for each side effect and sum all loss as total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(p_mat, plabel_num, nlabel_num, plabel_mat, nlabel_mat, weight):\n",
    "\n",
    "    loss, loss_list = 0, []\n",
    "\n",
    "    for i in range(p_mat.shape[1]):\n",
    "\n",
    "        pos_p = np.array(torch.nonzero(plabel_mat[:, i]))\n",
    "\n",
    "        pos_p = np.random.choice(pos_p.reshape(-1),\n",
    "                               int(0.5 * plabel_num[i]),\n",
    "                               replace=False)\n",
    "\n",
    "        plabel = torch.ones(1, int(0.5 * plabel_num[i]))\n",
    "        \n",
    "        pos_n = np.array(torch.nonzero(nlabel_mat[:, i]))\n",
    "\n",
    "        pos_n = np.random.choice(pos_n.reshape(-1),\n",
    "                               int(0.5 * nlabel_num[i]),\n",
    "                               replace=False)\n",
    "        \n",
    "        nlabel = torch.zeros((1, int(0.5 * nlabel_num[i])))\n",
    "        \n",
    "        pos = np.hstack((pos_p, pos_n))\n",
    "        \n",
    "        pos = torch.from_numpy(pos)\n",
    "        \n",
    "        label = torch.cat((plabel, nlabel), dim=1).reshape(-1)\n",
    "        \n",
    "        loss += F.binary_cross_entropy(p_mat[pos, i], label) * weight[i]\n",
    "        \n",
    "        loss_list.append((F.binary_cross_entropy(p_mat[pos, i], label) * weight[i]).item())\n",
    "\n",
    "    return loss, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regu():\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    for name,param in conduit_GNN.named_parameters():\n",
    "        \n",
    "        if 'conduit_update' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.005*l2_reg\n",
    "            \n",
    "        if 'node_update' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.005*l2_reg\n",
    "            \n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load feature of drug node and initialize CGNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_node_embedding = torch.load('/data/Decagon/dataset/all_node_embedding.pth')\n",
    "\n",
    "conduit_GNN = conduitGNN()\n",
    "\n",
    "optm = torch.optim.Adam(conduit_GNN.parameters(), lr=0.005)\n",
    "\n",
    "for name,param in conduit_GNN.named_parameters():\n",
    "    \n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load positive label and negative label for train sample, validation sample and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posi_label = np.load(\".\\\\polypharmacy side effect dataset\\\\train posi label.npy\")\n",
    "\n",
    "val_posi_label = np.load(\".\\\\polypharmacy side effect dataset\\\\val posi label.npy\")\n",
    "\n",
    "test_posi_label = np.load(\".\\\\polypharmacy side effect dataset\\\\test posi label.npy\")\n",
    "\n",
    "train_nega_label = np.load(\".\\\\polypharmacy side effect dataset\\\\train nega label.npy\")\n",
    "\n",
    "val_nega_label = np.load(\".\\\\polypharmacy side effect dataset\\\\val nega label.npy\")\n",
    "\n",
    "test_nega_label = np.load(\".\\\\polypharmacy side effect dataset\\\\test nega label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posi_label = torch.from_numpy(train_posi_label)\n",
    "\n",
    "val_posi_label = torch.from_numpy(val_posi_label)\n",
    "\n",
    "test_posi_label = torch.from_numpy(test_posi_label)\n",
    "\n",
    "train_nega_label = torch.from_numpy(train_nega_label)\n",
    "\n",
    "val_nega_label = torch.from_numpy(val_nega_label)\n",
    "\n",
    "test_nega_label = torch.from_numpy(test_nega_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi_label_num = []\n",
    "\n",
    "for c in range(train_posi_label.shape[1]):\n",
    "    \n",
    "    posi_label_num.append(torch.sum(train_posi_label[:, c]).item())\n",
    "    \n",
    "nega_label_num = []\n",
    "\n",
    "for c in range(train_nega_label.shape[1]):\n",
    "    \n",
    "    nega_label_num.append(torch.sum(train_nega_label[:, c]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following code to get the train acc, validation acc and test acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_list,test_acc_list = [],[]\n",
    "\n",
    "# exper_num = ''\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    loss,train_acc = 0,0\n",
    "        \n",
    "    conduit_GNN.train()\n",
    "    \n",
    "    conduit_GNN.cuda()\n",
    "    \n",
    "    start_loss_time = time.time()\n",
    "\n",
    "    train_output_matrix = conduit_GNN(all_node_embedding,\n",
    "                                      norm_adj_matrix.float().cuda(),\n",
    "                                      sample,\n",
    "                                      group,\n",
    "                                      use_divce = 1)\n",
    "        \n",
    "    train_acc_for_class = class_acc(train_output_matrix.cpu(),\n",
    "                              train_posi_label,\n",
    "                              train_nega_label)\n",
    "    \n",
    "#     file_acc = open('.\\\\'+exper_num+'\\\\'+str(epoch)+'train acc for class.txt','w')\n",
    "\n",
    "#     for i in train_acc_for_class:\n",
    "    \n",
    "#         file_acc.write(str(i.item())+'\\n')\n",
    "    \n",
    "#     file_acc.close()\n",
    "        \n",
    "    weight = 964 * [1]\n",
    "        \n",
    "    loss, loss_list = my_loss(train_output_matrix.cpu(),\n",
    "                        posi_label_num,\n",
    "                        nega_label_num,\n",
    "                        train_posi_label, \n",
    "                        train_nega_label,\n",
    "                        weight)\n",
    "        \n",
    "    loss = loss.cuda() + regu().cuda()\n",
    "    \n",
    "    end_loss_time = time.time()\n",
    "    \n",
    "    train_acc = np.mean(train_acc_for_class)\n",
    "    \n",
    "    print('Epoch :',epoch,'loss time',int(end_loss_time-start_loss_time),'loss:',loss.item(),'  train acc:',train_acc.item())\n",
    "    \n",
    "    start_optm_time = time.time()\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optm.step()\n",
    "    \n",
    "    end_optm_time = time.time()\n",
    "    \n",
    "    print('Epoch :',epoch,'optm time',int(end_optm_time-start_optm_time))\n",
    "    \n",
    "    if epoch > 5:#\n",
    "        \n",
    "        conduit_GNN.cpu()\n",
    "        \n",
    "        conduit_GNN.eval()\n",
    "      \n",
    "        valid_acc_for_class = class_acc(train_output_matrix.cpu(),\n",
    "                                                 val_posi_label,\n",
    "                                                 val_nega_label)\n",
    "        \n",
    "        validation_acc = np.mean(valid_acc_for_class)\n",
    "\n",
    "        if valid_acc_list != []:\n",
    "            \n",
    "            val_maxacc = max(valid_acc_list)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            val_maxacc = 0\n",
    "        \n",
    "        valid_acc_list.append(validation_acc)\n",
    "        \n",
    "        print('Epoch :',epoch,'validation acc',validation_acc.item())\n",
    "        \n",
    "        if validation_acc > val_maxacc:\n",
    "            \n",
    "#             torch.save(train_output_matrix.cpu(),\n",
    "#                         '.\\\\'+exper_num+'\\\\'+str(epoch)+'train_output_matrix.pth')\n",
    "        \n",
    "            test_acc_for_class = class_acc(train_output_matrix.cpu(), test_posi_label, test_nega_label)\n",
    "        \n",
    "            test_acc = np.mean(test_acc_for_class)\n",
    "        \n",
    "            test_acc_list.append(test_acc)\n",
    "        \n",
    "            print('Epoch :',epoch,'test acc',test_acc.item())\n",
    "            \n",
    "    del train_output_matrix\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "total_end_time = time.time()\n",
    "\n",
    "print('total time',int(total_end_time-total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
