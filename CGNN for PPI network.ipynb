{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selet subgraph of PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_num = 'graph1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the graph node of subgraph\n",
    "The index of graph node is from original graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_node = []\n",
    "\n",
    "file_graph_node = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' graph node.txt')\n",
    "\n",
    "for line in file_graph_node:\n",
    "    \n",
    "    graph_node.append(int(line))\n",
    "    \n",
    "file_graph_node.close()\n",
    "\n",
    "node_num = len(graph_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load adjecant matrix, norm adjecant matrix and degree matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppi_matrix = torch.load('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+'ppi adj matrix.pth')#ppi_matrix\n",
    "\n",
    "norm_ppi_matrix = torch.load('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+'ppi norm matrix.pth')#norm_ppi_matrix\n",
    "\n",
    "degree_matrix = torch.load('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+'degree matrix.pth')#degree_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train sample and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample,test_sample = [],[]\n",
    "\n",
    "file_train_sample = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' train_sample.txt')\n",
    "\n",
    "for line in file_train_sample:\n",
    "    \n",
    "    train_sample.append((int(line.split(' ')[0]),int(line.split(' ')[1].split('\\n')[0])))\n",
    "    \n",
    "file_train_sample.close()\n",
    "\n",
    "file_test_sample = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' test_sample.txt')\n",
    "\n",
    "for line in file_test_sample:\n",
    "    \n",
    "    test_sample.append((int(line.split(' ')[0]),int(line.split(' ')[1].split('\\n')[0])))\n",
    "    \n",
    "file_test_sample.close()\n",
    "\n",
    "print(len(train_sample),len(test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load conduit type group for train sample and test sample\n",
    "0 denotes 'LeftLeaning'(LL) conduit and 1 denotes 'RightLeaning'(RL) conduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group,test_group = [],[]\n",
    "\n",
    "file_train_group = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' train_group.txt')\n",
    "\n",
    "for line in file_train_group:\n",
    "    \n",
    "    train_group.append(int(line.split('\\n')[0]))\n",
    "    \n",
    "file_train_group.close()\n",
    "\n",
    "file_test_group = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' test_group.txt')\n",
    "\n",
    "for line in file_test_group:\n",
    "    \n",
    "    test_group.append(int(line.split('\\n')[0]))\n",
    "    \n",
    "file_test_group.close()\n",
    "\n",
    "print(len(train_group),len(test_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = []\n",
    "\n",
    "for i in range(node_num):\n",
    "    \n",
    "    degree_list.append(degree_matrix[i,i])\n",
    "    \n",
    "print(len(degree_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all data into the package structure to make it easier for CGNN to call the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_packge = [ppi_matrix.cuda(),norm_ppi_matrix.cuda(),train_sample,train_group,degree_list]\n",
    "\n",
    "test_package = [ppi_matrix.cpu(),norm_ppi_matrix.cpu(),test_sample,test_group,degree_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_degree(adj_matrix,node):\n",
    "    \n",
    "    return torch.sum(adj_matrix[node,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage1: node learning\n",
    "in_size is the dimension of the embedding of the previous layer ($l-1$).\n",
    "\n",
    "out_size is the dimension of the embedding of the current layer ($l$).\n",
    "\n",
    "old_node_embedding is the node embedding of the previous layer ($l-1$).\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node_learning(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_size, \n",
    "                 out_size):\n",
    "        super(node_learning, self).__init__()\n",
    "\n",
    "        self.nei_update1 = nn.Linear(in_size, out_size, bias=False)\n",
    "        \n",
    "        self.self_node_update1 = nn.Linear(in_size,\n",
    "                                           out_size,\n",
    "                                           bias=False)\n",
    "\n",
    "        self.nei_update2 = nn.Linear(in_size, out_size, bias=False)\n",
    "        \n",
    "        self.self_node_update2 = nn.Linear(in_size,\n",
    "                                           out_size,\n",
    "                                           bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                out_size,\n",
    "                package,\n",
    "                old_node_embedding,\n",
    "                use_divce):\n",
    "        \n",
    "        total_node = old_node_embedding.shape[0]\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            new_node_embedding = torch.zeros((total_node, out_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_node_embedding = torch.zeros((total_node, out_size)).cpu()\n",
    "\n",
    "        for node in range(total_node):\n",
    "            \n",
    "            nei_information, nei_gather = 0, 0\n",
    "            \n",
    "            self_information, total_information = 0, 0\n",
    "\n",
    "            if package[4][node] == 1:\n",
    "                \n",
    "                if get_node_degree(ppi_matrix, node) == 0:\n",
    "                    \n",
    "                    self_information += self.self_node_update1(\n",
    "                        old_node_embedding[node, :])\n",
    "                        \n",
    "                    total_information += F.relu(nei_information +\n",
    "                                                       self_information)\n",
    "                    \n",
    "                else:\n",
    "                    self_information += self.self_node_update1(\n",
    "                        old_node_embedding[node, :])\n",
    "                    \n",
    "                    nei_gather += torch.mm(package[1][node, :].reshape(1, -1),\n",
    "                                           old_node_embedding)\n",
    "                    \n",
    "                    nei_information += self.nei_update1(nei_gather)\n",
    "                    \n",
    "                    total_information += F.relu(nei_information +\n",
    "                                                       self_information)\n",
    "\n",
    "            else:\n",
    "                self_information += self.self_node_update2(\n",
    "                    old_node_embedding[node, :])\n",
    "                \n",
    "                nei_gather += torch.mm(package[1][node, :].reshape(1, -1),\n",
    "                                       old_node_embedding)\n",
    "                \n",
    "                nei_information += self.nei_update2(nei_gather)\n",
    "                \n",
    "                total_information += F.relu(nei_information +\n",
    "                                                   self_information)\n",
    "                \n",
    "            new_node_embedding[node, :] = torch.add(\n",
    "                new_node_embedding[node, :], total_information)\n",
    "            \n",
    "        return new_node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2: conduit node learning\n",
    "pre_size is the dimension of the embedding of the previous layer ($l-1$).\n",
    "\n",
    "next_size is the dimension of the embedding of the current layer ($l$).\n",
    "\n",
    "node_embedding is the node embedding of $l$-th layer.\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduit_node_learning(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pre_size,\n",
    "                 next_size):\n",
    "        \n",
    "        super(conduit_node_learning,self).__init__()\n",
    "        \n",
    "        self.left_gate1 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.right_gate1 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.left_gate2 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.right_gate2 = nn.Linear(pre_size,next_size,bias=False)\n",
    "\n",
    "        self.LL_conduit_update = nn.Linear(pre_size,next_size,bias=True)\n",
    "        \n",
    "        self.RL_conduit_update = nn.Linear(pre_size,next_size,bias=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                package,\n",
    "                next_size,\n",
    "                node_embedding,\n",
    "                use_divce):\n",
    "        \n",
    "        conduit_sample = package[2]\n",
    "        \n",
    "        index = list(range(len(conduit_sample)))\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros((len(conduit_sample),next_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros((len(conduit_sample),next_size)).cpu()\n",
    "            \n",
    "        for j in index:\n",
    "            \n",
    "            i = conduit_sample[j]\n",
    "            \n",
    "            left_gate,right_gate,conduit_embedding = 0,0,0\n",
    "            \n",
    "            combined_feature,gather_information = 0,0\n",
    "            \n",
    "            gather_information += torch.add(node_embedding[i[0],:],node_embedding[i[1],:])\n",
    "            \n",
    "            if package[3][j] == 1:\n",
    "                \n",
    "                left_gate += torch.sigmoid(self.left_gate1(node_embedding[i[0],:]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(self.right_gate1(node_embedding[i[1],:]))\n",
    "                \n",
    "                combined_feature += torch.tanh(self.LL_conduit_update(gather_information))\n",
    "                \n",
    "            if package[3][j] == 2:\n",
    "                \n",
    "                left_gate += torch.sigmoid(self.left_gate2(node_embedding[i[0],:]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(self.right_gate2(node_embedding[i[1],:]))\n",
    "                \n",
    "                combined_feature += torch.tanh(self.RL_conduit_update(gather_information))\n",
    "\n",
    "            conduit_embedding += left_gate*combined_feature + right_gate*combined_feature\n",
    "            \n",
    "            new_conduit_embedding[j,:] += conduit_embedding.reshape(-1)\n",
    "            \n",
    "        return new_conduit_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The framework of CGNN\n",
    "The dimensions of each layer are the same as those in paper.\n",
    "\n",
    "pre_node_embedding is the initial embedding of node.\n",
    "\n",
    "use_divce denotes whether CUDA is used.\n",
    "\n",
    "output_thred denotes whether outputing the conduit node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduitGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(conduitGNN,self).__init__()\n",
    "        \n",
    "        self.node_update_layer1 = node_learning(256,128)        \n",
    "        self.conduit_layer1 = conduit_node_learning(128,64)        \n",
    "        self.node_update_layer2 = node_learning(128,64)        \n",
    "        self.conduit_layer2 = conduit_node_learning(64,32)        \n",
    "        self.node_update_layer3 = node_learning(64,32)        \n",
    "        self.conduit_layer3 = conduit_node_learning(32,1)        \n",
    "        self.fuse_1 = nn.Linear(64,32,bias=True)        \n",
    "        self.fuse_2 = nn.Linear(32,1,bias=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                pre_node_embedding,\n",
    "                package,\n",
    "                use_divce,\n",
    "                output_thred):\n",
    "        \n",
    "        output_dict = {}\n",
    "        \n",
    "        use_divce = use_divce\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            output = torch.zeros((len(package[2]),1)).cuda()\n",
    "            \n",
    "            pre_node_embedding = pre_node_embedding.cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            output = torch.zeros((len(package[2]),1)).cpu()\n",
    "            \n",
    "            pre_node_embedding = pre_node_embedding.cpu()\n",
    "            \n",
    "        ##########\n",
    "        \n",
    "        node_embedding_1 = self.node_update_layer1(128,\n",
    "                                                   package,\n",
    "                                                   pre_node_embedding,\n",
    "                                                   use_divce)\n",
    "        \n",
    "        conduit_embedding_1 = self.conduit_layer1(package,\n",
    "                                                  64,\n",
    "                                                  node_embedding_1,\n",
    "                                                  use_divce)\n",
    "        \n",
    "        ##########\n",
    "        \n",
    "        node_embedding_2 = self.node_update_layer2(64,\n",
    "                                                   package,\n",
    "                                                   node_embedding_1,\n",
    "                                                   use_divce)\n",
    "        \n",
    "        conduit_embedding_2 = self.conduit_layer2(package,\n",
    "                                                  32,\n",
    "                                                  node_embedding_2,\n",
    "                                                  use_divce)\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        fused_embedding1 = torch.sigmoid(self.fuse_1(conduit_embedding_1)+conduit_embedding_2)\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        node_embedding_3 = self.node_update_layer3(32,\n",
    "                                                   package,\n",
    "                                                   node_embedding_2,\n",
    "                                                   use_divce)\n",
    "        \n",
    "        conduit_embedding_3 = self.conduit_layer3(package,\n",
    "                                                  1,\n",
    "                                                  node_embedding_3,\n",
    "                                                  use_divce)\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        fused_embedding2 = torch.sigmoid(self.fuse_2(fused_embedding1)+conduit_embedding_3)\n",
    "        \n",
    "        #########\n",
    "        \n",
    "        output += fused_embedding2\n",
    "        \n",
    "        if output_thred == 1:\n",
    "            output_dict['conduit_embedding_1'] = conduit_embedding_1\n",
    "            output_dict['conduit_embedding_2'] = conduit_embedding_2\n",
    "            output_dict['fused_embedding1'] = fused_embedding1\n",
    "            return output_dict, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Com_acc(output,lab):\n",
    "\n",
    "    result = output.ge(0.5).float() == lab.reshape(-1,1)\n",
    "    \n",
    "    acc = result.float().mean()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Com_recall(output,lab):\n",
    "    \n",
    "    pred = output.ge(0.5).float()\n",
    "    \n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    posi_index = np.where(np.array(lab)==1)[0]\n",
    "    \n",
    "    posi_pred,posi_label = np.array(pred)[posi_index],np.array(lab)[posi_index]\n",
    "    \n",
    "    recall = np.sum(posi_pred == posi_label,dtype = np.float64)/(posi_index).shape[0]\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regu():\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    for name,param in conduit_GNN.named_parameters():\n",
    "        \n",
    "        if 'conduit_update' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.005*l2_reg\n",
    "            \n",
    "        if 'node_update' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.005*l2_reg\n",
    "            \n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing CGNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduit_GNN = conduitGNN().cuda()\n",
    "\n",
    "optm = torch.optim.Adam(conduit_GNN.parameters(), lr=0.01)\n",
    "\n",
    "BCE = nn.BCELoss()\n",
    "\n",
    "for name,param in conduit_GNN.named_parameters():\n",
    "    \n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load initial node embedding for protein\n",
    "select initial embedding matrix of subgraph using graph_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_node_embedding = torch.load('.\\\\homogeneous network\\\\PPI\\\\all_node_embedding.pth')\n",
    "\n",
    "graph_node_tensor = torch.tensor(graph_node)\n",
    "\n",
    "graph_node_embedding = all_node_embedding[graph_node_tensor,:]\n",
    "\n",
    "print(graph_node_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train label and test label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label,test_label = [],[]\n",
    "\n",
    "file_train_label = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' train_label.txt')\n",
    "\n",
    "for line in file_train_label:\n",
    "    \n",
    "    train_label.append(int(line))\n",
    "    \n",
    "file_train_label.close()\n",
    "\n",
    "file_test_label = open('.\\\\homogeneous network\\\\PPI\\\\'+graph_num+' test_label.txt')\n",
    "\n",
    "for line in file_test_label:\n",
    "    \n",
    "    test_label.append(int(line))\n",
    "    \n",
    "file_test_label.close()\n",
    "\n",
    "\n",
    "tensor_all_train_label = torch.tensor(train_label).float()\n",
    "\n",
    "tensor_all_test_label = torch.tensor(test_label).float()\n",
    "\n",
    "print(len(train_label),len(test_label))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following code to get the train acc and test acc, recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list,test_acc_list = [],[]\n",
    "test_recall_list = []\n",
    "test_epoch = []\n",
    "# exper_num = ''\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    loss,train_acc = 0,0\n",
    "    \n",
    "    conduit_GNN.train()\n",
    "    \n",
    "    conduit_GNN.cuda()\n",
    "    \n",
    "    train_start_time = time.time()\n",
    "    \n",
    "    train_output_matrix = conduit_GNN(graph_node_embedding,\n",
    "                                      train_packge,\n",
    "                                      1,\n",
    "                                      0)\n",
    "    \n",
    "    train_acc = Com_acc(train_output_matrix,tensor_all_train_label.cuda())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        \n",
    "        max_train_acc = train_acc.item()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        max_train_acc = max(train_acc_list)\n",
    "    \n",
    "    train_acc_list.append(train_acc.item())\n",
    "    \n",
    "    train_end_time = time.time()\n",
    "    \n",
    "    print('Epoch : %d, train time : %d, train acc : %.4f'%(epoch, int(train_end_time-train_start_time), train_acc.item()))\n",
    "    \n",
    "    if epoch > 4 and train_acc.item() > max_train_acc:\n",
    "        \n",
    "        conduit_GNN.cpu()\n",
    "        \n",
    "        conduit_GNN.eval()\n",
    "        \n",
    "        test_start_time = time.time()\n",
    "        \n",
    "        test_output_matrix = conduit_GNN(graph_node_embedding,\n",
    "                                         test_package,\n",
    "                                         0,\n",
    "                                         0)\n",
    "        \n",
    "        test_acc = Com_acc(test_output_matrix,tensor_all_test_label)\n",
    "        test_recall = Com_recall(test_output_matrix,tensor_all_test_label)\n",
    "        \n",
    "        test_acc_list.append(test_acc.item())\n",
    "        test_recall_list.append(test_recall)\n",
    "        test_epoch.append(epoch)\n",
    "        \n",
    "        test_end_time = time.time()\n",
    "        print('Epoch :',epoch,'test time',int(test_end_time-test_start_time),'test acc',test_acc,'test recall',test_recall)\n",
    "\n",
    "#         torch.save(train_output_matrix, '.\\\\'+graph_num+'\\\\'+exper_num+' '+str(epoch)+'train output.pth')\n",
    "        \n",
    "#         state = {'CGNN':conduit_GNN.state_dict(),'optimizer':optm.state_dict(),'epoch':epoch}\n",
    "#         torch.save(state, '.\\\\'+graph_num+'\\\\'+exper_num+' '+str(epoch)+'model parameter.pth')\n",
    "\n",
    "#         torch.save(test_output_matrix,'.\\\\'+graph_num+'\\\\'+exper_num+' '+str(epoch)+'test output.pth')\n",
    "        del test_output_matrix\n",
    "    \n",
    "    loss_start_time = time.time()\n",
    "    \n",
    "    conduit_GNN.train()\n",
    "    conduit_GNN.cuda()\n",
    "    \n",
    "    loss = BCE(train_output_matrix.reshape(-1),tensor_all_train_label.cuda()) + regu().cuda()\n",
    "    optm.zero_grad()\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    \n",
    "    loss_end_time = time.time()\n",
    "    \n",
    "    print('Epoch :',epoch,'loss: ',loss,'loss time: ',int(loss_end_time - loss_start_time))\n",
    "    del train_output_matrix\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "total_end_time = time.time()\n",
    "print('total time ',int(total_end_time - total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
