{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load adjacent matrix and weight matrix of gene and disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_adj = torch.load('.\\heterogeneous network\\lncRNA-disease\\lnc_adj.pth')\n",
    "\n",
    "gene_wei = torch.load('.\\heterogeneous network\\lncRNA-disease\\lnc_wei.pth')\n",
    "\n",
    "dis_adj = torch.load('.\\heterogeneous network\\lncRNA-disease\\dis_adj.pth')\n",
    "\n",
    "dis_wei = torch.load('.\\heterogeneous network\\lncRNA-disease\\dis_wei.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train sample, validation sample and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample,valid_sample,test_sample = [],[],[]\n",
    "\n",
    "file_lncdis_train = open('.\\heterogeneous network\\lncRNA-disease\\train_lncdis.txt')\n",
    "\n",
    "for i in file_lncdis_train:\n",
    "    \n",
    "    train_sample.append((int(i.split()[0]),int(i.split()[1].split('\\n')[0])))\n",
    "    \n",
    "file_lncdis_train.close()\n",
    "\n",
    "file_lncdis_valid = open('.\\heterogeneous network\\lncRNA-disease\\valid_lncdis.txt')\n",
    "\n",
    "for i in file_lncdis_valid:\n",
    "    \n",
    "    valid_sample.append((int(i.split()[0]),int(i.split()[1].split('\\n')[0])))\n",
    "    \n",
    "file_lncdis_valid.close()\n",
    "\n",
    "file_lncdis_test = open('.\\heterogeneous network\\lncRNA-disease\\test_lncdis.txt')\n",
    "\n",
    "for i in file_lncdis_test:\n",
    "    \n",
    "    test_sample.append((int(i.split()[0]),int(i.split()[1].split('\\n')[0])))\n",
    "    \n",
    "file_lncdis_test.close()\n",
    "\n",
    "print(len(train_sample),len(valid_sample),len(test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load conduit type group for train sample, validation sample and test sample\n",
    "0 denote scarce conduit, 1 denote unilateral_conduit and 2 denote bilateral_conduit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group, valid_group, test_group = [], [], []\n",
    "\n",
    "file_train_group = open('.\\heterogeneous network\\lncRNA-disease\\ld_train group.txt')\n",
    "\n",
    "for line in file_train_group:\n",
    "\n",
    "    train_group.append(int(line))\n",
    "\n",
    "file_train_group.close()\n",
    "\n",
    "file_valid_group = open('.\\heterogeneous network\\lncRNA-disease\\ld_valid group.txt')\n",
    "\n",
    "for line in file_valid_group:\n",
    "\n",
    "    valid_group.append(int(line))\n",
    "\n",
    "file_valid_group.close()\n",
    "\n",
    "file_test_group = open('.\\heterogeneous network\\lncRNA-disease\\ld_test group.txt')\n",
    "\n",
    "for line in file_test_group:\n",
    "\n",
    "    test_group.append(int(line))\n",
    "\n",
    "file_test_group.close()\n",
    "\n",
    "print(len(train_group), len(valid_group), len(test_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all data into the package structure to make it easier for CGNN to call the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_package = [[gene_adj,gene_wei],[dis_adj,dis_wei],valid_sample,valid_group]\n",
    "\n",
    "test_package = [[gene_adj,gene_wei],[dis_adj,dis_wei],test_sample,test_group]\n",
    "\n",
    "train_package = [[gene_adj.cuda(),gene_wei.cuda()],[dis_adj.cuda(),dis_wei.cuda()],train_sample,train_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_degree(adj_matrix,node):\n",
    "    \n",
    "    return torch.sum(adj_matrix[node,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage1: node learning\n",
    "in_size is the dimension of the embedding of the previous layer ($l-1$).\n",
    "\n",
    "out_size is the dimension of the embedding of the current layer ($l$).\n",
    "\n",
    "pos is the index of network ($0$ denotes gene-gene network, $1$ denotes disease-disease network).\n",
    "\n",
    "old_node_embedding is the embedding of the previous layer ($l-1$).\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node_learning(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_size,\n",
    "                 out_size):\n",
    "        \n",
    "        super(node_learning,self).__init__()\n",
    "        \n",
    "        self.nei_update1 = nn.Linear(in_size,out_size,bias=False)\n",
    "        \n",
    "        self.self_node_update1 = nn.Linear(in_size,out_size,bias=False)\n",
    "        \n",
    "        self.nei_update2 = nn.Linear(in_size,out_size,bias=False)\n",
    "        \n",
    "        self.self_node_update2 = nn.Linear(in_size,out_size,bias=False)        \n",
    "        \n",
    "    def forward(self,\n",
    "                pos,\n",
    "                out_size,\n",
    "                package,\n",
    "                old_node_embedding,\n",
    "                use_divce):\n",
    "        \n",
    "        total_node = old_node_embedding[pos].shape[0]\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            new_node_embedding = torch.zeros((total_node,out_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_node_embedding = torch.zeros((total_node,out_size)).cpu()\n",
    "            \n",
    "        for node in range(total_node):\n",
    "            \n",
    "            nei_information,nei_gather = 0,0\n",
    "            \n",
    "            self_information = 0\n",
    "            \n",
    "            if pos == 0:\n",
    "                \n",
    "                self_information += self.self_node_update1(old_node_embedding[pos][node,:].reshape(-1))\n",
    "                    \n",
    "                if get_node_degree(package[pos][0],node) > 0:\n",
    "                    \n",
    "                    nei_gather = torch.mm(package[pos][1][node,:].reshape(1,-1),old_node_embedding[pos])\n",
    "                    \n",
    "                    nei_information += self.nei_update1(nei_gather.reshape(-1))\n",
    "\n",
    "                total_information = F.relu(nei_information + self_information)\n",
    "            else:\n",
    "                \n",
    "                self_information += self.self_node_update2(old_node_embedding[pos][node,:].reshape(-1))\n",
    "                    \n",
    "                if get_node_degree(package[pos][0],node) > 0:\n",
    "                    \n",
    "                    nei_gather = torch.mm(package[pos][1][node,:].reshape(1,-1),old_node_embedding[pos])\n",
    "                    \n",
    "                    nei_information += self.nei_update2(nei_gather.reshape(-1))\n",
    "\n",
    "                total_information = F.relu(nei_information + self_information)\n",
    "                \n",
    "            new_node_embedding[node,:] = torch.add(new_node_embedding[node,:],total_information)\n",
    "            \n",
    "        return new_node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2: conduit node learning\n",
    "pre_size is the dimension of the embedding of the previous layer ($l-1$).\n",
    "\n",
    "next_size is the dimension of the embedding of the current layer ($l$).\n",
    "\n",
    "node_embedding is a list containing the embedding of gene node and disease node.\n",
    "\n",
    "list[0] stores embeddings of gene nodes and list[1] stores embeddings of disease nodes.\n",
    "\n",
    "use_divce denotes whether CUDA is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduit_node_learning(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pre_size,\n",
    "                 next_size):\n",
    "        \n",
    "        super(conduit_node_learning,self).__init__()\n",
    "        \n",
    "        self.left_gate_update1 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.right_gate_update1 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.left_gate_update2 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.right_gate_update2 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.left_gate_update3 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.right_gate_update3 = nn.Linear(pre_size,next_size,bias=False)\n",
    "        \n",
    "        self.conduit_update1 = nn.Linear(pre_size,next_size,bias=True)\n",
    "        \n",
    "        self.conduit_update2 = nn.Linear(pre_size,next_size,bias=True)\n",
    "        \n",
    "        self.conduit_update3 = nn.Linear(pre_size,next_size,bias=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                package,\n",
    "                next_size,\n",
    "                node_embedding,\n",
    "                use_divce):\n",
    "        \n",
    "        conduit_sample = package[2]\n",
    "        \n",
    "        index = list(range(len(conduit_sample)))\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros((len(conduit_sample),next_size)).cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_conduit_embedding = torch.zeros((len(conduit_sample),next_size)).cpu()\n",
    "            \n",
    "        for j in index:\n",
    "            \n",
    "            i = conduit_sample[j]\n",
    "            \n",
    "            left_gate,right_gate,conduit_embedding = 0,0,0\n",
    "            \n",
    "            combined_feature,gather_information = 0,0\n",
    "            \n",
    "            gather_information = torch.add(node_embedding[0][i[0],:],node_embedding[1][i[1],:])\n",
    "            \n",
    "            if package[3][j] == 0:\n",
    "                \n",
    "                left_gate += torch.sigmoid(self.left_gate_update1(node_embedding[0][i[0],:]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(self.right_gate_update1(node_embedding[1][i[1],:]))\n",
    "                \n",
    "                combined_feature += torch.tanh(self.conduit_update1(gather_information))\n",
    "                \n",
    "            if package[3][j] == 1:\n",
    "                \n",
    "                left_gate += torch.sigmoid(self.left_gate_update2(node_embedding[0][i[0],:]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(self.right_gate_update2(node_embedding[1][i[1],:]))\n",
    "                \n",
    "                combined_feature += torch.tanh(self.conduit_update2(gather_information))\n",
    "                \n",
    "            if package[3][j] == 2:\n",
    "                \n",
    "                left_gate += torch.sigmoid(self.left_gate_update3(node_embedding[0][i[0],:]))\n",
    "                \n",
    "                right_gate += torch.sigmoid(self.right_gate_update3(node_embedding[1][i[1],:]))\n",
    "                \n",
    "                combined_feature += torch.tanh(self.conduit_update3(gather_information))\n",
    "    \n",
    "            conduit_embedding += left_gate*combined_feature + right_gate*combined_feature\n",
    "        \n",
    "            new_conduit_embedding[j,:] += conduit_embedding.reshape(-1)\n",
    "            \n",
    "        return new_conduit_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The framework of CGNN\n",
    "The dimensions of each layer are the same as those in paper.\n",
    "\n",
    "gene_node_embedding is the initial embedding of gene node.\n",
    "\n",
    "dis_node_embedding is the initial embedding of disease node.\n",
    "\n",
    "use_divce denotes whether CUDA is used.\n",
    "\n",
    "output_thred denotes whether outputing conduit node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conduitGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(conduitGNN,self).__init__()\n",
    "        \n",
    "        self.node_update_layer1 = node_learning(256,128)\n",
    "        self.conduit_layer1 = conduit_node_learning(128,64)\n",
    "        self.node_update_layer2 = node_learning(128,64)\n",
    "        self.conduit_layer2 = conduit_node_learning(64,32)\n",
    "        self.node_update_layer3 = node_learning(64,32)\n",
    "        self.conduit_layer3 = conduit_node_learning(32,1)\n",
    "        self.fuse_1 = nn.Linear(64,32,bias=True)\n",
    "        self.fuse_2 = nn.Linear(32,1,bias=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                gene_node_embedding,\n",
    "                dis_node_embedding,\n",
    "                package,\n",
    "                use_divce,\n",
    "                output_thred):#\n",
    "        \n",
    "        use_divce = use_divce\n",
    "        \n",
    "        if use_divce == 1:\n",
    "            \n",
    "            output = torch.zeros((len(package[2]),1)).cuda() \n",
    "            \n",
    "            gene_node_embedding = gene_node_embedding.cuda()\n",
    "            \n",
    "            dis_node_embedding = dis_node_embedding.cuda()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            output = torch.zeros((len(package[2]),1)).cpu()\n",
    "            \n",
    "            gene_node_embedding = gene_node_embedding.cpu()\n",
    "            \n",
    "            dis_node_embedding = dis_node_embedding.cpu()\n",
    "            \n",
    "        output_dict = {}\n",
    "            \n",
    "        init_node_embedding[0] = gene_node_embedding\n",
    "        \n",
    "        init_node_embedding[1] = dis_node_embedding\n",
    "        \n",
    "        ##########\n",
    "        node_embedding_1_left = self.node_update_layer1(0,\n",
    "                                                        128,\n",
    "                                                        package,\n",
    "                                                        init_node_embedding,\n",
    "                                                        use_divce)\n",
    "        \n",
    "        node_embedding_1_right = self.node_update_layer1(1,\n",
    "                                                         128,\n",
    "                                                         package,\n",
    "                                                         init_node_embedding,\n",
    "                                                         use_divce)\n",
    "\n",
    "        node_embedding_1 = [node_embedding_1_left,node_embedding_1_right]\n",
    "        \n",
    "        conduit_embedding_1 = self.conduit_layer1(package,\n",
    "                                                  64,\n",
    "                                                  node_embedding_1,\n",
    "                                                  use_divce)\n",
    "        \n",
    "        ##########\n",
    "        node_embedding_2_left = self.node_update_layer2(0,\n",
    "                                                        64,\n",
    "                                                        package,\n",
    "                                                        node_embedding_1,\n",
    "                                                        use_divce)\n",
    "        \n",
    "        node_embedding_2_right = self.node_update_layer2(1,\n",
    "                                                         64,\n",
    "                                                         package,\n",
    "                                                         node_embedding_1,\n",
    "                                                         use_divce)\n",
    "        \n",
    "        node_embedding_2 = [node_embedding_2_left,node_embedding_2_right]\n",
    "        \n",
    "        conduit_embedding_2 = self.conduit_layer2(package,\n",
    "                                                  32,\n",
    "                                                  node_embedding_2,\n",
    "                                                  use_divce)\n",
    "\n",
    "        fused_embedding1 = torch.sigmoid(self.fuse_1(conduit_embedding_1)+conduit_embedding_2)\n",
    "\n",
    "        #########\n",
    "        node_embedding_3_left = self.node_update_layer3(0,\n",
    "                                                        32,\n",
    "                                                        package,\n",
    "                                                        node_embedding_2,\n",
    "                                                        use_divce)\n",
    "        \n",
    "        node_embedding_3_right = self.node_update_layer3(1,\n",
    "                                                         32,\n",
    "                                                         package,\n",
    "                                                         node_embedding_2,\n",
    "                                                         use_divce)\n",
    "        \n",
    "        node_embedding_3 = [node_embedding_3_left,node_embedding_3_right]\n",
    "        \n",
    "        conduit_embedding_3 = self.conduit_layer3(package,\n",
    "                                                  1,\n",
    "                                                  node_embedding_3,\n",
    "                                                  use_divce)\n",
    "        \n",
    "        fused_embedding2 = torch.sigmoid(self.fuse_2(fused_embedding1)+conduit_embedding_3)\n",
    "        \n",
    "        #########\n",
    "        output += fused_embedding2\n",
    "        \n",
    "        if output_thred == 1:\n",
    "            output_dict['conduit_embedding_1'] = conduit_embedding_1\n",
    "            output_dict['conduit_embedding_2'] = conduit_embedding_2\n",
    "            output_dict['fused_embedding1'] = fused_embedding1\n",
    "            return output_dict, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Com_acc(output,lab):\n",
    "\n",
    "    result = output.ge(0.5).float() == lab.reshape(-1,1)\n",
    "    \n",
    "    acc = result.float().mean()\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Com_recall(output,lab):\n",
    "\n",
    "    pred = output.ge(0.5).float()\n",
    "    \n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    posi_index = np.where(np.array(lab)==1)[0]\n",
    "    \n",
    "    posi_pred,posi_label = np.array(pred)[posi_index],np.array(lab)[posi_index]\n",
    "    \n",
    "    recall = np.sum(posi_pred == posi_label,dtype = np.float64)/(posi_index).shape[0]\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regu():\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    for name,param in conduit_GNN.named_parameters():\n",
    "        \n",
    "        if 'fuse' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.02*l2_reg\n",
    "            \n",
    "        if 'conduit_update' in name and 'weight' in name:\n",
    "            \n",
    "            l2_reg = torch.norm(param,p=2)\n",
    "            \n",
    "            reg_loss += 0.005*l2_reg\n",
    "            \n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load initial node embedding for gene and disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_node_embedding = torch.load('.\\heterogeneous network\\lncRNA-disease\\lnc_node_embedding.pth')\n",
    "\n",
    "dis_node_embedding = torch.load('.\\heterogeneous network\\lncRNA-disease\\disForlnc_node_embedding.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train label, validation label and test label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label,valid_label,test_label = [],[],[]\n",
    "\n",
    "file_lncdis_train = open('.\\heterogeneous network\\lncRNA-disease\\ld_train_label.txt')\n",
    "\n",
    "for i in file_lncdis_train:\n",
    "    \n",
    "    train_label.append(int(i))\n",
    "    \n",
    "file_lncdis_train.close()\n",
    "\n",
    "file_lncdis_valid = open('.\\heterogeneous network\\lncRNA-disease\\ld_valid_label.txt')\n",
    "\n",
    "for i in file_lncdis_valid:\n",
    "    \n",
    "    valid_label.append(int(i))\n",
    "    \n",
    "file_lncdis_valid.close()\n",
    "\n",
    "file_lncdis_test = open('.\\heterogeneous network\\lncRNA-disease\\ld_test_label.txt')\n",
    "\n",
    "for i in file_lncdis_test:\n",
    "    \n",
    "    test_label.append(int(i))\n",
    "    \n",
    "file_lncdis_test.close()\n",
    "\n",
    "tensor_train_label = torch.tensor(train_label).float().cuda()\n",
    "\n",
    "tensor_vali_label = torch.tensor(valid_label).float()\n",
    "\n",
    "tensor_test_label = torch.tensor(test_label).float()\n",
    "\n",
    "print(len(train_label),len(valid_label),len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing CGNN model\n",
    "init_node_embedding is a list to store initial embedding of gene and disease in conduitGNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conduit_GNN = conduitGNN().cuda()\n",
    "\n",
    "init_node_embedding = 2*['']\n",
    "\n",
    "optm = torch.optim.Adam(conduit_GNN.parameters(), lr=0.01)\n",
    "\n",
    "BCE = nn.BCELoss()\n",
    "\n",
    "for name,param in conduit_GNN.named_parameters():\n",
    "    \n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following code to get the train acc, validation acc and test acc, recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_list,test_acc_list = [],[]\n",
    "test_recall_list = []\n",
    "# exper_num = ''\n",
    "total_start_time = time.time()\n",
    "max_valid_acc = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    loss,train_acc = 0,0\n",
    "\n",
    "    conduit_GNN.train()\n",
    "    conduit_GNN.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_output_matrix = conduit_GNN(gene_node_embedding,\n",
    "                                      dis_node_embedding,\n",
    "                                      train_package,\n",
    "                                      use_divce = 1,\n",
    "                                      output_thred = 0)#\n",
    "    \n",
    "    loss = BCE(train_output_matrix.reshape(-1),tensor_train_label.cuda()) + regu().cuda()\n",
    "    \n",
    "    optm.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optm.step()\n",
    "    \n",
    "    train_acc = Com_acc(train_output_matrix,tensor_train_label.cuda())\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    print('Epoch : %d, time : %d, loss : %.4f, train acc : %.4f'%\\\n",
    "          (epoch, int(end_time-start_time), loss.item(), train_acc.item()))\n",
    "    \n",
    "    if epoch > 10:#\n",
    "        \n",
    "        conduit_GNN.cpu()\n",
    "        \n",
    "        conduit_GNN.eval()\n",
    "        \n",
    "        vali_output_matrix = conduit_GNN(gene_node_embedding,\n",
    "                                         dis_node_embedding,\n",
    "                                         validation_package,\n",
    "                                         use_divce = 0,\n",
    "                                         output_thred = 0)#\n",
    "        \n",
    "        validation_acc = Com_acc(vali_output_matrix,tensor_vali_label)\n",
    "        \n",
    "        valid_acc_list.append(validation_acc)\n",
    "        \n",
    "        print('Epoch : %d, validation acc : %.4f'%\\\n",
    "              (epoch, validation_acc.item()))\n",
    "        \n",
    "        if validation_acc > max_valid_acc:\n",
    "            \n",
    "            max_valid_acc = validation_acc\n",
    "            \n",
    "#             state = {'CGNN':conduit_GNN.state_dict(),'optimizer':optm.state_dict(),'epoch':epoch}\n",
    "            \n",
    "#             torch.save(state, exper_num+' '+str(epoch)+' model parameter.pth')\n",
    "        \n",
    "            test_output_matrix = conduit_GNN(gene_node_embedding,\n",
    "                                             dis_node_embedding,\n",
    "                                             test_package,\n",
    "                                             use_divce = 0,\n",
    "                                             output_thred = 0)#\n",
    "\n",
    "            test_acc = Com_acc(test_output_matrix,tensor_test_label)#\n",
    "\n",
    "            test_recall = Com_recall(test_output_matrix,tensor_test_label)\n",
    "\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "            test_recall_list.append(test_recall)\n",
    "\n",
    "            print('Epoch : %d, test acc : %.4f, test recall : %.4f'\\\n",
    "                  %(epoch, test_acc.item(), test_recall))\n",
    "#             torch.save(test_output_matrix, exper_num+' '+str(epoch)+' test probability output.pth')\n",
    "       \n",
    "total_end_time = time.time()\n",
    "print('total time : %d'%int(total_end_time-total_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
